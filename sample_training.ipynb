{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Callable, Union, Optional\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from scripts.default_setting import *\n",
    "\n",
    "from GGUtils.utils.path import do_or_load, GetAbsolutePath\n",
    "from GGDL.utils import get_verbose, get_process_id, code_test, set_seed_everything, make_basic_directory, GetDevice, Option\n",
    "from GGDL.idx_dict.key_df import make_basic_key_df, binary_label_convertor\n",
    "from GGDL.idx_dict.make_dict import StratifiedIndexdict\n",
    "from GGDL.data_loader.classification import BasicDataset\n",
    "from GGDL.model.vision import TimmHelper\n",
    "from GGDL.model.optimzer import OptimHelper, SchedulerHelper\n",
    "from GGDL.data_loader.utils import LabelDtype\n",
    "from GGDL.pipeline.log import Log\n",
    "from GGDL.metrics.classification import MetricsClassification\n",
    "\n",
    "from GGImgMorph.scenario import sample_augment      # 증강 알고리즘\n",
    "\n",
    "from GGStatify.descript import frequency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option\n",
    "### Option 1. basic option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu 상태 확인\n",
    "GET_DEVICE = GetDevice()\n",
    "GET_DEVICE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 기본 설정\n",
    "#######################################\n",
    "# Process 상태 출력\n",
    "VERBOSE = True\n",
    "# code test 여부\n",
    "CODE_TEST = False\n",
    "# log save 여부\n",
    "SAVE_LOG = True\n",
    "# GPU 번호\n",
    "GPU = 0\n",
    "# process 진행 중 생성되는 파일들이 저장되는 초기 디렉터리 초기화 여부\n",
    "MAKE_NEW_DEFAULT_DIR = True\n",
    "# idx_dict을 새로 생성할지 여부\n",
    "MAKE_NEW_IDX_DICT = True\n",
    "# 모델의 종류\n",
    "MODEL_TYPE = \"classification\"\n",
    "# 이진 분류 여부\n",
    "IS_BINARY = True\n",
    "\n",
    "\n",
    "# Data 설정\n",
    "#######################################\n",
    "# image의 크기\n",
    "IMG_SIZE = 224\n",
    "# index dictionary 생성 방식\n",
    "K_FOLD = 5                  # k-fold의 크기 (Stratified sampling)\n",
    "TEST_RATIO = 0.2            # test dataset ratio\n",
    "VALID_RATIO = 0.1           # validation dataset ratio, None인 경우 생성하지 않음\n",
    "\n",
    "\n",
    "# 자동 설정\n",
    "#######################################\n",
    "_VERBOSE = get_verbose(verbose=VERBOSE)\n",
    "_PROCESS_ID = get_process_id(verbose=_VERBOSE)\n",
    "_CODE_TEST = code_test(do=CODE_TEST, test_ratio=0.1, verbose=_VERBOSE)\n",
    "\n",
    "# device 설정\n",
    "_DEVICE = GET_DEVICE(GPU)\n",
    "torch.cuda.set_device(_DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm에서 사용하고자 하는 모델의 이름을 찾는다.\n",
    "model_name_ptn = r\"efficient.+_b3\"\n",
    "TimmHelper.search(model_name_ptn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 관련 설정\n",
    "#######################################\n",
    "# model 이름\n",
    "MODEL_NAME = 'efficientnet_b3.ra2_in1k'\n",
    "# pre-training 여부\n",
    "PRETRAINED = True\n",
    "# Fine tuning 방식\n",
    "    # 0:FullFineTuning\n",
    "    # 1:FixedFeatureExtractor\n",
    "    # 2:PartialLayerFreezing\n",
    "    # 3:FreezeNUnfreeze\n",
    "TUNER_DICT = {\n",
    "    'how':2,\n",
    "    'freezing_ratio':0.9\n",
    "}\n",
    "# image의 channel 크기\n",
    "IMG_CHANNEL = 3\n",
    "# model이 추론할 class의 크기\n",
    "CLASS_SIZE = 0\n",
    "# 모델 추론을 위한 metrics\n",
    "METRICS = MetricsClassification(is_binary=True, threshold=0.5)\n",
    "\n",
    "# header 관련 설정\n",
    "#######################################\n",
    "from scripts.header import custom_binary_header, EXTRA_ACTIVATION_FN\n",
    "\n",
    "# backbone 모델의 새로운 header\n",
    "CUSTOM_HEAD_FN = custom_binary_header\n",
    "# 손실 함수\n",
    "LOSS_FN = nn.BCEWithLogitsLoss()\n",
    "# label의 dtype을 loss function에 맞게 설정\n",
    "LABEL_DTYPE_INS = LabelDtype(loss_fn=LOSS_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3. pipe line setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe line 내 각종 설정\n",
    "#######################################\n",
    "# AMP 사용 여부\n",
    "USE_AMP = True\n",
    "# Grad clipping 사용 여부\n",
    "USE_CLIPPING = True\n",
    "# Early Stopping 관련 설정\n",
    "EARLY_STOPPING = {\n",
    "    \"patience\":10,\n",
    "    \"delta\":0.0,\n",
    "    \"target\":\"loss\",\n",
    "    \"auto_remove\":True,\n",
    "    \"best_model_dir\":f\"{RESULT}/{_PROCESS_ID}\"\n",
    "}\n",
    "\n",
    "# data loader 관련 설정\n",
    "#######################################\n",
    "DATASET_CLASS = BasicDataset           # dataset의 class\n",
    "TRAINSET_KWARGS = {\n",
    "    \"augments\":sample_augment, \n",
    "    \"resize\":IMG_SIZE,\n",
    "    \"resize_how\":0,                  # resize 방법\n",
    "    \"resize_how_list\":[2, 3, 4],     # 무작위 resize 시, 방법의 list\n",
    "    \"resize_padding_color\":\"random\"  # # resize padding 시, pixel의 색\n",
    "}\n",
    "VALIDSET_KWARGS = {\n",
    "    \"resize\":IMG_SIZE,\n",
    "    \"resize_how\":2,                  # resize 방법\n",
    "    \"resize_padding_color\":\"black\"  # # resize padding 시, pixel의 색\n",
    "}\n",
    "WORKER = 4                          # DataLoader의 num_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 4. Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter 관련 설정\n",
    "#######################################\n",
    "HP_DICT = {\n",
    "    'epochs':1000,\n",
    "    'batch_size':64,\n",
    "    \n",
    "    # Optimizer\n",
    "    'lr':0.00001,\n",
    "    'betas':(0.9, 0.999),\n",
    "    'eps':1e-08,\n",
    "    \n",
    "    # Scheduler\n",
    "    'T_0':20,\n",
    "    'T_mult':1,\n",
    "    'eta_min':0.000001,\n",
    "    \n",
    "    # clipping\n",
    "    'clipping_max_norm':5\n",
    "}\n",
    "\n",
    "# Optimizer 정의\n",
    "OPTIM_HELPER = OptimHelper(name='Adam', hp_dict=HP_DICT)\n",
    "# Scheduler 정의\n",
    "SCHEDULE_HELPER = SchedulerHelper(name='CosineAnnealingWarmRestarts', hp_dict=HP_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "### Process 1. make key_df\n",
    "* key_df는 img의 절대 경로(\"path\")와 label(\"label\") 두 개의 컬럼으로 구성된 DataFrame 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 정보\n",
    "TRAIN_SET = \"/mnt/d/rawdata/dogs-vs-cats/train/\"        # train set의 경로\n",
    "TEST_SET = \"/mnt/d/rawdata/dogs-vs-cats/test1/\"         # test set의 경로\n",
    "\n",
    "\n",
    "# key_df 생성\n",
    "path_list = GetAbsolutePath(None).get_all_path(parents_path=TRAIN_SET)\n",
    "key_df = make_basic_key_df(\n",
    "    paths=path_list,\n",
    "    labels=[re.split(r\".+/\", i, maxsplit=1)[1].split('.')[0] for i in path_list]\n",
    ")\n",
    "# label을 이진 분류로 변환\n",
    "key_df['label'] = binary_label_convertor(array=key_df['label'], positive_class='dog')\n",
    "\n",
    "# 이해를 돕기 위한 key_df 출력\n",
    "key_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 2. make idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_dict의 경로\n",
    "IDX_DICT_PATH = f\"{SOURCE}/idx_dict.pickle\"\n",
    "\n",
    "# 기초 디렉터리 생성\n",
    "make_basic_directory(source=SOURCE, estop_dir=ESPOINT_DIR, log=LOG, result=RESULT, make_new=MAKE_NEW_DEFAULT_DIR)\n",
    "\n",
    "# Process log 설정\n",
    "LOG_INS = Log(log_dir=LOG, process_id=_PROCESS_ID, model_type=MODEL_TYPE, save_log=SAVE_LOG)\n",
    "\n",
    "# index dictionary 생성\n",
    "make_index_dict = StratifiedIndexdict(columns=['label'], is_binary=IS_BINARY)\n",
    "IDX_DICT = do_or_load(\n",
    "    savepath=IDX_DICT_PATH, makes_new=MAKE_NEW_IDX_DICT, \n",
    "    fn=make_index_dict,\n",
    "    key_df=key_df, k_fold_size=K_FOLD, test_ratio=TEST_RATIO, valid_ratio=VALID_RATIO, code_test=_CODE_TEST\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 3. make option instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = Option(\n",
    "    process_id=_PROCESS_ID, verbose=_VERBOSE, log_ins=LOG_INS,\n",
    "\n",
    "    model_name=MODEL_NAME, pretrained=PRETRAINED, device=_DEVICE, use_amp=USE_AMP, use_clipping=USE_CLIPPING,\n",
    "    img_channel=IMG_CHANNEL, class_size=CLASS_SIZE, custom_header=CUSTOM_HEAD_FN, extra_activation_fn=EXTRA_ACTIVATION_FN,\n",
    "    metrics=METRICS,\n",
    "\n",
    "    optim_helper=OPTIM_HELPER, scheduler_helper=SCHEDULE_HELPER, loss_fn=LOSS_FN, label_dtype_fn=LABEL_DTYPE_INS, \n",
    "    tuner_dict=TUNER_DICT, hp_dict=HP_DICT, \n",
    "\n",
    "    dataset_class=DATASET_CLASS, trainset_kwargs=TRAINSET_KWARGS, validset_kwargs=VALIDSET_KWARGS, worker=WORKER,\n",
    "\n",
    "    idx_dict=IDX_DICT, results_parents=RESULT, espoint_parents=f\"{SOURCE}/{ESPOINT_DIR}\",\n",
    "    \n",
    "    early_stopping=EARLY_STOPPING\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 4. model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GGDL.pipeline.pipeline import BackPropagation, EarlyStopping\n",
    "from GGDL.model.fine_tuning import Tuner\n",
    "from GGDL.model.vision import Classification as Model\n",
    "from GGDL.data_loader.classification import GetLoader\n",
    "from GGDL.pipeline.fit.classification import Classification as Fit\n",
    "from GGUtils.utils.path import new_dir_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 전 모든 seed 고정\n",
    "set_seed_everything(seed=SEED)\n",
    "\n",
    "# k-fold cross validation\n",
    "for k in option.idx_dict.keys():\n",
    "    k_idx_dict = option.idx_dict[k]     # k-fold에 대한 idx_dict\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option.log_ins.k = k        # log의 k값 설정\n",
    "\n",
    "# 결과가 저장될 디렉터리 생성\n",
    "new_dir_maker(dir_path=f\"{option.results_parents}/{option.process_id}\")\n",
    "\n",
    "# Data Loader 정의\n",
    "loader = GetLoader(\n",
    "    idx_dict=k_idx_dict, dataset_class=option.dataset_class, \n",
    "    batch_size=option.hp_dict['batch_size'], workers=option.worker\n",
    ")\n",
    "loader(key=\"train\", **option.trainset_kwargs)\n",
    "loader(key=\"test\", **option.validset_kwargs)\n",
    "if \"valid\" in k_idx_dict:\n",
    "    loader(key=\"valid\", **option.validset_kwargs)\n",
    "\n",
    "# model 정의\n",
    "model = Model(\n",
    "    model_name=option.model_name, pretrained=option.pretrained, \n",
    "    channel=option.img_channel, class_size=option.class_size,\n",
    "    custom_head_fn=option.custom_header\n",
    ").to(option.device)\n",
    "\n",
    "# Optimizer 설정\n",
    "optimizer = option.optim_helper(param=model.parameters())\n",
    "back_propagation = BackPropagation(\n",
    "    optimizer=optimizer, use_amp=option.use_amp, \n",
    "    use_clipping=option.use_clipping, \n",
    "    max_norm=option.hp_dict['clipping_max_norm'] if 'clipping_max_norm' in option.hp_dict else None,\n",
    "    device=option.device\n",
    ")\n",
    "# scheduler 설정\n",
    "option.scheduler_helper(optimizer)\n",
    "\n",
    "# Early Stopping 설정\n",
    "if option.early_stopping is not None:\n",
    "    early_stopping = EarlyStopping(\n",
    "        model=model, path=f\"{option.espoint_parents}/{option.process_id}.pt\", \n",
    "        **option.early_stopping\n",
    "    )\n",
    "    early_stopping.k = k\n",
    "else:\n",
    "    early_stopping = None\n",
    "\n",
    "# Fine tuning 방법 정의\n",
    "tuner = Tuner(model, **option.tuner_dict)\n",
    "tuner(epoch=0)      # model parameter 초기 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GGDL.pipeline.fit.utils import get_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INS = Fit(\n",
    "    model=model, loader=loader, optimizer=optimizer, \n",
    "    back_propagation=back_propagation, early_stopping=early_stopping, option=option\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_log_txt = None\n",
    "for bar_txt, epoch in TEST_INS.pbar_ins(range(option.hp_dict['epochs'])):\n",
    "\n",
    "    # Epochs 시작 시 log txt\n",
    "    epoch_log_txt = TEST_INS.option.log_ins.epoch_log_txt(epoch_log_txt=epoch_log_txt, bar_txt=bar_txt)\n",
    "\n",
    "    # model training\n",
    "    train_loss, train_acc = TEST_INS._fit_iterator(epoch, epoch_log_txt)\n",
    "\n",
    "    # validation and early stopping\n",
    "    if TEST_INS.loader.valid is not None:\n",
    "        valid_loss, valid_acc, _ = TEST_INS.inference(loader=TEST_INS.loader.valid)\n",
    "        \n",
    "        # Early stopping\n",
    "        if TEST_INS.early_stopping_mask:\n",
    "            valid_score = TEST_INS.early_stopping.validation_score(loss=valid_loss, acc=valid_acc)\n",
    "            es_log_txt = TEST_INS.early_stopping(epoch, score=valid_score)\n",
    "    else:\n",
    "        valid_loss, valid_acc = (None, None)\n",
    "        \n",
    "    # scheduler 조정\n",
    "    scheduler_score = valid_loss if valid_loss is not None else train_loss\n",
    "    TEST_INS.option.scheduler_helper.epoch_step(score=scheduler_score)\n",
    "        \n",
    "    # epoch 내 log 출력\n",
    "    epoch_log_txt = TEST_INS._epoch_log(\n",
    "        bar_txt, es_log_txt, epoch, \n",
    "        train_acc, train_loss, valid_acc, valid_loss\n",
    "    )\n",
    "    # Early stopping stop\n",
    "    if TEST_INS.early_stopping.stop:\n",
    "        break\n",
    "\n",
    "# 학습 종료 후, test set에 대한 지표 생성 및 log 출력\n",
    "test_predict_dict = TEST_INS.end_of_fit(epoch, bar_txt, train_acc, train_loss, valid_acc, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_e4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
