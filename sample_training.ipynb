{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Callable, Union, Optional\n",
    "\n",
    "import sys, os, cv2, re\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scripts.default_setting import *\n",
    "\n",
    "from GGUtils.utils.path import do_or_load, GetAbsolutePath\n",
    "from GGUtils.img.viewer import show_img, show_imgs\n",
    "from GGUtils.utils.utils import time_checker\n",
    "from GGDL.utils import set_seed_everything, make_basic_directory, tensor_to_img, GetDevice, Option\n",
    "from GGDL.idx_dict.key_df import make_basic_key_df, binary_label_convertor\n",
    "from GGDL.idx_dict.make_dict import make_stratified_idx_dict\n",
    "from GGDL.data_loader.dataset import ImgDataset, GetLoader, show_dataset_img\n",
    "from GGDL.model.vision import Classification, TimmHelper\n",
    "from GGDL.model.fine_tuning import Tuner\n",
    "from GGDL.model.optimzer import Optim, LabelDtype\n",
    "from GGDL.pipeline.pipeline import BackPropagation\n",
    "\n",
    "from GGImgMorph.scenario import sample_augment      # 증강 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process id\n",
    "PROCESS_ID = os.getpid()\n",
    "print(\"해당 process의 id: \", PROCESS_ID)\n",
    "\n",
    "# 학습 상태 출력\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm에서 사용하고자 하는 모델의 이름을 찾는다.\n",
    "_model_name_ptn = \"mobilenetv1_.+_r224\"\n",
    "TimmHelper.search(_model_name_ptn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 관련 설정\n",
    "MODEL_NAME = 'mobilenetv1_100.ra4_e3600_r224_in1k'      # baseline model\n",
    "IMG_CHANNEL = 3              # image의 channel 크기\n",
    "CLASS_SIZE = 0               # model이 추론할 class의 크기\n",
    "PRETRAINED = True\n",
    "USE_AMP = True               # AMP 사용 여부\n",
    "USE_CLIPPING = True          # Grad clipping 사용 여부\n",
    "\n",
    "\n",
    "# Custom header\n",
    "class HeaderBlock(nn.Module):\n",
    "    def __init__(self, input_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(HeaderBlock, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(input_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.gelu(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def custom_header(x:int)->nn.Module:\n",
    "    \"\"\"\n",
    "    Pre-Activation Batch Normalization\n",
    "    - 깊은 Backbone 모델의 header이므로, Internal Covariate Shift 문제 해결을 위해 사용\n",
    "\n",
    "    BottleNeck\n",
    "    - 정보를 확장하여 중요한 정보만 남겨, 계산 효율성을 유지하면서 높은 표현력 제공\n",
    "    \"\"\"\n",
    "    header = nn.Sequential(\n",
    "        HeaderBlock(input_dim=x, output_dim=1024, dropout_prob=0.1),\n",
    "        HeaderBlock(input_dim=1024, output_dim=512, dropout_prob=0.3),\n",
    "        HeaderBlock(input_dim=512, output_dim=128, dropout_prob=0.5),\n",
    "        nn.Linear(128, 1)\n",
    "    )\n",
    "    return header\n",
    "\n",
    "CUSTOM_HEAD_FN = custom_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. pipe line setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu 상태 확인\n",
    "GET_DEVICE = GetDevice()\n",
    "GET_DEVICE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device 설정\n",
    "GPU = 0\n",
    "DEVICE = GET_DEVICE(GPU)\n",
    "torch.cuda.set_device(DEVICE)\n",
    "\n",
    "# data loader 관련 설정\n",
    "DATASET_CLASS = ImgDataset          # dataset의 class\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 224                      # 고정된 image의 크기\n",
    "RESIZE_HOW = 2                      # resize 방법\n",
    "RESIZE_HOW_LIST = [2, 3, 4]         # 무작위 resize 시, 방법의 list\n",
    "RESIZE_PADDING_COLOR = \"random\"     # resize padding 시, pixel의 색\n",
    "WORKER = 0                          # DataLoader의 num_worker\n",
    "\n",
    "# early stopping 시 경로\n",
    "ESTOP_PATH = f\"{SOURCE}/{ESPOINT_DIR}/process_{PROCESS_ID}\"\n",
    "\n",
    "# process 진행 중 생성되는 파일들이 저장되는 초기 디렉터리 초기화 여부\n",
    "MAKE_NEW_DEFAULT_DIR = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3. Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_DICT = {\n",
    "    'lr':0.0001,\n",
    "    'betas':(0.9, 0.999),\n",
    "    'eps':1e-08,\n",
    "    'clipping_max_norm':5\n",
    "}\n",
    "OPTIM_INS = Optim(name='Adam', hp_dict=HP_DICT)\n",
    "\n",
    "\n",
    "LOSS_FN = nn.BCEWithLogitsLoss()\n",
    "LABEL_TYPE_INS = LabelDtype(loss_fn=LOSS_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "### Process 1. make key_df\n",
    "* key_df는 img의 절대 경로(\"path\")와 label(\"label\") 두 개의 컬럼으로 구성된 DataFrame 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 정보\n",
    "TRAIN_SET = \"/mnt/d/rawdata/dogs-vs-cats/train/\"        # train set의 경로\n",
    "TEST_SET = \"/mnt/d/rawdata/dogs-vs-cats/test1/\"         # test set의 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_df 생성\n",
    "path_list = GetAbsolutePath(None).get_all_path(parents_path=TRAIN_SET)\n",
    "key_df = make_basic_key_df(\n",
    "    paths=path_list,\n",
    "    labels=[re.split(r\".+/\", i, maxsplit=1)[1].split('.')[0] for i in path_list]\n",
    ")\n",
    "# label을 이진 분류로 변환\n",
    "key_df['label'] = binary_label_convertor(array=key_df['label'], positive_class='dog')\n",
    "\n",
    "# 이해를 돕기 위한 key_df 출력\n",
    "key_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 2. make idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAKE_NEW_IDX_DICT = True                            # idx_dict을 새로 생성할지 여부\n",
    "IDX_DICT_PATH = f\"{SOURCE}/idx_dict.pickle\"         # idx_dict이 저장될 경로\n",
    "\n",
    "# idx_dict 생성 방식\n",
    "K_SIZE = 5                  # k-fold의 크기 (Stratified sampling)\n",
    "TEST_RATIO = 0.2            # test dataset ratio\n",
    "VALID_RATIO = 0.1           # validation dataset ratio, None인 경우 생성하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기초 디렉터리 생성\n",
    "make_basic_directory(source=SOURCE, estop_dir=ESPOINT_DIR, log=LOG, result=RESULT, make_new=MAKE_NEW_DEFAULT_DIR)\n",
    "\n",
    "# idx_dict 생성\n",
    "IDX_DICT = do_or_load(\n",
    "    savepath=IDX_DICT_PATH, makes_new=MAKE_NEW_IDX_DICT, \n",
    "    fn=make_stratified_idx_dict,\n",
    "    key_df=key_df, stratified_columns=['label'], is_binary=True,\n",
    "    path_col='path', label_col='label', \n",
    "    k_size=K_SIZE, test_ratio=TEST_RATIO, valid_ratio=VALID_RATIO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 3. make option instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = Option(\n",
    "    process_id=PROCESS_ID,\n",
    "    model_name=MODEL_NAME, pretrained=PRETRAINED, device=DEVICE, idx_dict=IDX_DICT,\n",
    "    optimizer=OPTIM_INS, loss_fn=LOSS_FN, label_type_fn=LABEL_TYPE_INS, use_amp=USE_AMP, use_clipping=USE_CLIPPING,\n",
    "    img_size=IMG_SIZE, resize_how=RESIZE_HOW, resize_how_list=RESIZE_HOW_LIST, resize_padding_color=RESIZE_PADDING_COLOR,\n",
    "    dataset_class=DATASET_CLASS, augments=sample_augment, batch_size=BATCH_SIZE, worker=WORKER,\n",
    "    img_channel=IMG_CHANNEL, class_size=CLASS_SIZE, custom_header=CUSTOM_HEAD_FN,\n",
    "    tuner_how=2, hp_dict=HP_DICT, verbose=VERBOSE,\n",
    "    log_parents=f\"{LOG}\", results_parents=f\"{RESULT}\", espoint_parents=f\"{SOURCE}/{ESPOINT_DIR}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 4. model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GGUtils.utils.path import new_dir_maker, make_null_list_pickle, load_pickle, save_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log:\n",
    "    def __init__(self, log_dir, process_id):\n",
    "\n",
    "        self.parents_path = f\"{log_dir}/{process_id}\"\n",
    "        self.iterator_py = f\"{self.parents_path}/iterator.py\"\n",
    "        self.epoch_py = f\"{self.parents_path}/epoch.py\"\n",
    "        self.k = None\n",
    "\n",
    "\n",
    "    def make(self):\n",
    "        new_dir_maker(self.parents_path)\n",
    "        make_null_list_pickle(self.iterator_py)\n",
    "        make_null_list_pickle(self.epoch_py)\n",
    "\n",
    "\n",
    "    def read(self, open_epoch_py:bool=False)->List[Any]:\n",
    "        pickle_path = self.epoch_py if open_epoch_py else self.iterator_py\n",
    "        return load_pickle(pickle_path)\n",
    "    \n",
    "\n",
    "    def write(self, data:List[Any], open_epoch_py:bool=False):\n",
    "        pickle_path = self.epoch_py if open_epoch_py else self.iterator_py\n",
    "        save_pickle(data, pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 전 모든 seed 고정\n",
    "set_seed_everything(seed=SEED)\n",
    "\n",
    "log_ins = Log(log_dir=option.log_parents, process_id=option.process_id)\n",
    "log_ins.make()\n",
    "\n",
    "for k in option.idx_dict.keys():\n",
    "    \n",
    "    k_idx_dict = option.idx_dict[k]     # k-fold에 대한 idx_dict\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log 설정\n",
    "log_ins.k = k\n",
    "\n",
    "# Data Loader 정의\n",
    "loader = GetLoader(\n",
    "    dataset_class=option.dataset_class, idx_dict=k_idx_dict,\n",
    "    augments=option.augments, batch_size=option.batch_size, workers=option.worker, \n",
    "    resize=option.img_size, resize_how=option.resize_how, resize_how_list=option.resize_how_list,\n",
    "    resize_padding_color = option.resize_padding_color\n",
    ")\n",
    "\n",
    "# model 정의\n",
    "model = Classification(\n",
    "    model_name=option.model_name, pretrained=option.pretrained, \n",
    "    channel=option.img_channel, class_size=option.class_size,\n",
    "    custom_head_fn=option.custom_header\n",
    ").to(option.device)\n",
    "\n",
    "# Optimizer 설정\n",
    "optimizer = option.optimizer(param=model.parameters())\n",
    "grad_scaler = torch.GradScaler(device=option.device) if option.use_amp else None\n",
    "back_propagation = BackPropagation(\n",
    "    optimizer=optimizer, use_amp=option.use_amp, grad_scaler=grad_scaler,\n",
    "    use_clipping=option.use_clipping, \n",
    "    max_norm=option.hp_dict['clipping_max_norm'] if 'clipping_max_norm' in option.hp_dict else None\n",
    ")\n",
    "\n",
    "# Fine tuning 방법 정의\n",
    "tuner = Tuner(model, how=2, freezing_ratio=0.9)\n",
    "tuner(epoch=0)      # model parameter 초기 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification:\n",
    "    def __init__(self, model, loader, optimizer, back_propagation, log_ins, option):\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.optimizer = optimizer\n",
    "        self.back_propagation = back_propagation\n",
    "        self.log_ins = log_ins\n",
    "        self.option = option\n",
    "\n",
    "\n",
    "    def _fit_epoch(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _fit_iterator(self)->Tuple[float, str]:\n",
    "        start = time.time()\n",
    "        loss_list = []\n",
    "        for imgs, labels in loader.train:\n",
    "\n",
    "            # load to device\n",
    "            imgs = imgs.to(self.option.device)\n",
    "            # label dtype을 loss_fn에 맞게 수정 및 shape 등 변형\n",
    "            labels = option.label_type_fn(labels).reshape(-1, 1).to(self.option.device)\n",
    "\n",
    "            # AMP\n",
    "            with torch.autocast(device_type=option.device, enabled=self.option.use_amp):\n",
    "                output = model(imgs)\n",
    "                loss = option.loss_fn(output, labels)\n",
    "                \n",
    "            # back propagation\n",
    "            back_propagation(loss)\n",
    "\n",
    "            # loss log\n",
    "            loss_list.append(loss.item())\n",
    "        \n",
    "        # iterator의 loss 평균, 종료 시간 출력\n",
    "        return np.mean(loss_list), time_checker(start)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INS = Classification(\n",
    "    model=model, loader=loader.train, optimizer=optimizer, \n",
    "    back_propagation=back_propagation, log_ins=log_ins, option=option\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "loss_list = []\n",
    "for imgs, labels in loader.train:\n",
    "\n",
    "    # load to device\n",
    "    imgs = imgs.to(TEST_INS.option.device)\n",
    "    # label dtype을 loss_fn에 맞게 수정 및 shape 등 변형\n",
    "    labels = option.label_type_fn(labels).reshape(-1, 1).to(TEST_INS.option.device)\n",
    "\n",
    "    # AMP\n",
    "    with torch.autocast(device_type=option.device, enabled=TEST_INS.option.use_amp):\n",
    "        output = model(imgs)\n",
    "        loss = option.loss_fn(output, labels)\n",
    "        \n",
    "    # back propagation\n",
    "    back_propagation(loss)\n",
    "\n",
    "    # loss log\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Callable, Union, Optional\n",
    "\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "from GGUtils.utils.utils import current_time, decimal_seconds_to_time_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressbar(header:str, i:int, iter_size:int, bar_size:int=40, number_size:Optional[int]=None, done_txt=\"#\", rest_txt=\".\")->str:\n",
    "    \"\"\"\n",
    "    iterator에 대하여, header, i, iteration의 크기 등을 기반으로 progressbar txt를 생성한다.\n",
    "    example) header [##############...........................] (12/100)\n",
    "\n",
    "    Args:\n",
    "        header (str): progressbar 앞에 붙는 iterator의 주제\n",
    "        i (int): iterator에서 출력된 순서(enumerate()로 출력된 n값)\n",
    "        iter_size (int): iterator의 크기\n",
    "        bar_size (int): progressbar의 크기. Defaults to 40.\n",
    "        number_size (Optional[int], optional): progressbar의 정수로된 진행 상황의 정수 고정 크기. Defaults to None.\n",
    "            - (    1/ 1000) 처럼 정수의 앞과 뒤의 크기를 맞추기 위한 parameter\n",
    "            - None인 경우, iter_size를 기반으로 계산: len(str(iter_size)) + 1\n",
    "            - +1은 여백을 위해 추가\n",
    "        done_txt (str, optional): progressbar에서 진행된 부분의 txt. Defaults to \"#\".\n",
    "        rest_txt (str, optional): progressbar에서 진행되지 않은 부분의 txt. Defaults to \".\".\n",
    "\n",
    "    Returns:\n",
    "        str: _description_\n",
    "    \"\"\"\n",
    "    # number_size가 None인 경우, 현재의 iter_size로 계산\n",
    "    number_size = len(str(iter_size)) + 1 if number_size is None else number_size\n",
    "    \n",
    "    # bar txt\n",
    "    done_size = int(bar_size*i/iter_size)                               # iter에서 진행된 크기\n",
    "    rest_size = bar_size - done_size                                    # 진행되지 않은 크기\n",
    "    bar_txt = \"%s[%s%s]\" % (header, (done_size+1)*done_txt, rest_size*rest_txt)   # header를 추가하여 bar_txt 생성\n",
    "    \n",
    "    # number txt\n",
    "    count = f\"({i:>{number_size}}/{iter_size:>{number_size}})\"\n",
    "    return bar_txt + count\n",
    "    \n",
    "\n",
    "\n",
    "class ProgressBar:\n",
    "    def __init__(self, header:str, bar_size:int=40, delta_format:str=\"{:.3f}\", log_save:bool=False):\n",
    "        self.header = header\n",
    "        self.bar_size = bar_size\n",
    "        self.delta_format = delta_format\n",
    "        self.log_save = log_save\n",
    "\n",
    "        # 객체 생성 시 고정 변수\n",
    "        self.delta_deque = deque(maxlen=2)      # 순간 변화 \n",
    "        self.eta_deque = deque(maxlen=20)       # 평균 변화\n",
    "\n",
    "        # callable 시 고정 변수\n",
    "        self.start_time = None                  # iteration 시작 당시 시간\n",
    "        self.iter_size = None                   # iteration의 크기\n",
    "        self.number_size = None                 # [  1 / 1000] 의 정수 크기\n",
    "\n",
    "\n",
    "    def __call__(self, iter):\n",
    "        self._callable_initial_variable(iter)       # Callable 시, instance variable 설정\n",
    "        for i, item in enumerate(iter):\n",
    "            bar_txt = progressbar(header=self.header, i=i, iter_size=self.iter_size, bar_size=self.bar_size, number_size=self.number_size)\n",
    "            yield item, bar_txt\n",
    "    \n",
    "\n",
    "    def _callable_initial_variable(self, iter):\n",
    "        self.iter_size = len(iter)\n",
    "        self.number_size = len(str(self.iter_size)) + 1\n",
    "        self.start_time = time.time()\n",
    "        self.delta_deque.append(self.start_time)\n",
    "    \n",
    "    \n",
    "    def end_of_iterator(self, *txts, verbose:bool=False):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def _print_progressbar(self, txts):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "class Time:\n",
    "    def __init__(self, delta_deque:deque, eta_deque:deque, iter_size:int, start_time:float, format:str=\"{:.3f}\"):\n",
    "        \"\"\"\n",
    "        ProgressBar class와 함께 사용되며, iterator 내부에서 시간 관련 정보를 측정한다.\n",
    "        \n",
    "        측정하는 정보는 다음과 같다.\n",
    "            1. spent: iterator 시작부터 Callable까지 소모 시간\n",
    "            2. eta: eta_deque를 기반으로 예상 완료 시간\n",
    "            3. iter s/it: iterator의 평균 소모 시간\n",
    "            4. current: 해당 log가 출력된 현재 시간\n",
    "            5. eta_deque[-1]: Callable되었을 때, itarator의 소모 시간\n",
    "\n",
    "        Args:\n",
    "            delta_deque (deque): ProcessBar class에서 iterator 각 cycle에 대한 소모 시간 측정을 위한 deque\n",
    "            eta_deque (deque): ProcessBar class에서 iterator 각 cycle의 소모 시간이 누적되는 deque\n",
    "                - 예상 완료 시간 산출 목적\n",
    "            iter_size (int): ProcessBar class에 입력된 iterator의 크기\n",
    "            start_time (float): ProcessBar class에서 iterator가 시작되었을 때의 시간\n",
    "            format (str): 소수점이 있는 시간의 소수점 조정. Defaults to \"{:.3f}\".\n",
    "        \"\"\"\n",
    "        self.delta_deque = delta_deque\n",
    "        self.eta_deque = eta_deque\n",
    "        self.iter_size = iter_size\n",
    "        self.start_time = start_time\n",
    "        self.format = format\n",
    "        \n",
    "        \n",
    "    def __call__(self, i:int)->Tuple[str, Dict[str, Union[str, float]]]:\n",
    "        \"\"\"\n",
    "        iterator 안에서 Callable 되었을 때를 기준으로 시간 관련 정보 생성\n",
    "        \n",
    "        생성 정보는 두 가지로 다음과 같다.\n",
    "        1. text: [time] spend: 1 day, 6:28:55.33, eta: 0:00:00.00, 133.6778s/it, current: 2024.07.24 08:16:26]\n",
    "        2. dictionary: {\"current\": \"2024.07.24 08:16:26\", \"spent\":\"1 day, 6:28:55.33\", \"iteration\":133.6778}\n",
    "\n",
    "        Args:\n",
    "            i (int): 현재 iterator의 순서\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, Dict[str, Union[str, float]]]: 시간 log text, 시간 log의 dictionary\n",
    "        \"\"\"\n",
    "        # 시간 deque 추가\n",
    "        self.delta_deque.append(time.time())                                        # callable 실행 시, 시간 추가\n",
    "        self.eta_deque.append(float(self.delta_deque[1] - self.delta_deque[0]))     # 한 iterater의 소모 시간 추가\n",
    "        # 주요 시간 정보 생성\n",
    "        current, iter_mean, eta, spent = self._make_interest_time(i)\n",
    "        # 정리 및 출력\n",
    "        time_txt = self._make_to_time_txt(current, iter_mean, eta, spent)\n",
    "        log_dict = self._make_to_time_txt(current, spent)\n",
    "        return time_txt, log_dict\n",
    "        \n",
    "        \n",
    "    def _make_interest_time(self, i:int)->Tuple[str, float, float, float]:\n",
    "        \"\"\"\n",
    "        관심 시간 변수들을 계산한다.\n",
    "        current: 현재 시간\n",
    "        iter_mean: eta_deque의 평균 시간(iterator의 평균 시간)\n",
    "        eta: iterator 종료까지 예상 시간\n",
    "        spent: iterator 시작부터 현재까지 소모된 시간\n",
    "        \"\"\"\n",
    "        current = current_time()                        # log 생성을 위한 현재 시간 문자열\n",
    "        iter_mean = np.mean(self.eta_deque)             # eta_deque 기준, 1 iter 당 평균 시간 계산\n",
    "        eta = (self.iter_size - i) * iter_mean          # 예상 남은 시간\n",
    "        spent = self.delta_deque[-1] - self.start_time  # itorator 시작부터 callable까지 소모된 시간\n",
    "        return current, iter_mean, eta, spent\n",
    "        \n",
    "        \n",
    "    def _make_to_time_txt(self, current:str, iter_mean:float, eta:float, spent:float):\n",
    "        \"\"\"\n",
    "        self._make_interest_time()로 계산된 관심 시간 변수들을 text로 정리\n",
    "        \"\"\"\n",
    "        spent_txt = decimal_seconds_to_time_string(decimal_s=spent)\n",
    "        eta_txt = decimal_seconds_to_time_string(decimal_s=eta)\n",
    "        clean_iter_mean = self.format.format(iter_mean)\n",
    "        return f\"[time] spent: {spent_txt}, eta: {eta_txt}, {clean_iter_mean}s/it, current: {current}\"\n",
    "    \n",
    "    \n",
    "    def _log_dict(self, current:str, spent:str):\n",
    "        \"\"\"\n",
    "        주요 시간 정보들을 log로 저장\n",
    "        \"\"\"\n",
    "        log_dict = {\n",
    "            \"current\":current,\n",
    "            \"spent\":spent,\n",
    "            \"iteration\":self.eta_deque[-1]\n",
    "        }\n",
    "        return log_dict\n",
    "    \n",
    "        \n",
    "        \n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "# class Scheduler:\n",
    "#     def __init__(self, name:str, hp_dict:Dict[str, Any]):\n",
    "#         self.methods = {\n",
    "#             'LambdaLR': lr_scheduler.LambdaLR,\n",
    "#             'MultiplicativeLR': lr_scheduler.MultiplicativeLR,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             'StepLR': lr_scheduler.StepLR,\n",
    "#             'MultiStepLR': lr_scheduler.MultiStepLR,\n",
    "#             'ExponentialLR': lr_scheduler.ExponentialLR,\n",
    "#             'CosineAnnealingLR': lr_scheduler.CosineAnnealingLR,\n",
    "#             'CyclicLR': lr_scheduler.CyclicLR,\n",
    "#             'CosineAnnealingWarmRestarts': lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "#             'ReduceLROnPlateau': lr_scheduler.ReduceLROnPlateau,\n",
    "#             'OneCycleLR': lr_scheduler.OneCycleLR,\n",
    "#             'PolynomialLR': lr_scheduler.PolynomialLR,\n",
    "#             'LinearLR': lr_scheduler.LinearLR,\n",
    "#             'ConstantLR': lr_scheduler.ConstantLR,\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             'ChainedScheduler': lr_scheduler.ChainedScheduler,\n",
    "#             'SequentialLR': lr_scheduler.SequentialLR,\n",
    "            \n",
    "#         }\n",
    "\n",
    "\n",
    "#     def lambdalr(self):\n",
    "#         pass\n",
    "\n",
    "\n",
    "# class Schedulers:\n",
    "#     def __init__(self):\n",
    "#         pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_e4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
