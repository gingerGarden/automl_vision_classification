{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Callable, Union, Optional\n",
    "\n",
    "import sys, os, cv2, re\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scripts.default_setting import *\n",
    "\n",
    "from GGUtils.utils.path import do_or_load, GetAbsolutePath\n",
    "from GGUtils.img.viewer import show_img, show_imgs\n",
    "from GGUtils.utils.utils import time_checker\n",
    "from GGUtils.log.progressbar import ProgressBar\n",
    "from GGDL.utils import set_seed_everything, make_basic_directory, GetDevice, Option\n",
    "from GGDL.idx_dict.key_df import make_basic_key_df, binary_label_convertor\n",
    "from GGDL.idx_dict.make_dict import make_stratified_idx_dict\n",
    "from GGDL.data_loader.classification import ImgDataset, GetLoader, show_dataset_img, show_batch_imgs\n",
    "from GGDL.model.vision import Classification, TimmHelper\n",
    "from GGDL.model.fine_tuning import Tuner\n",
    "from GGDL.model.optimzer import Optim, LabelDtype\n",
    "from GGDL.pipeline.pipeline import BackPropagation\n",
    "from GGDL.pipeline.log import Log\n",
    "from GGDL.metrics.classification import MetricsClassification\n",
    "\n",
    "from GGImgMorph.scenario import sample_augment      # 증강 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "해당 process의 id:  419802\n",
      "Code Test: True, 전체 데이터의 0.1로 프로세스를 처리한다.\n"
     ]
    }
   ],
   "source": [
    "# process id\n",
    "PROCESS_ID = os.getpid()\n",
    "print(\"해당 process의 id: \", PROCESS_ID)\n",
    "\n",
    "# 학습 상태 출력\n",
    "VERBOSE = 0\n",
    "\n",
    "# log save 여부\n",
    "SAVE_LOG = True\n",
    "\n",
    "# 모델 학습 및 추론을 Test code로 진행할지\n",
    "# CODE_TEST가 None이 아닌 실수인 경우, 전체 데이터의 양을 n%로 감소시켜 전체 프로세스를 진행한다.\n",
    "CODE_TEST = 0.1\n",
    "if CODE_TEST is not None:\n",
    "    print(f\"Code Test: True, 전체 데이터의 {CODE_TEST}로 프로세스를 처리한다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mobilenetv1_100.ra4_e3600_r224_in1k',\n",
       " 'mobilenetv1_100h.ra4_e3600_r224_in1k',\n",
       " 'mobilenetv1_125.ra4_e3600_r224_in1k']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# timm에서 사용하고자 하는 모델의 이름을 찾는다.\n",
    "_model_name_ptn = \"mobilenetv1_.+_r224\"\n",
    "TimmHelper.search(_model_name_ptn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 관련 설정\n",
    "MODEL_NAME = 'mobilenetv1_100.ra4_e3600_r224_in1k'\n",
    "IMG_CHANNEL = 3              # image의 channel 크기\n",
    "CLASS_SIZE = 0               # model이 추론할 class의 크기\n",
    "PRETRAINED = True            # pre-training 여부\n",
    "USE_AMP = True               # AMP 사용 여부\n",
    "USE_CLIPPING = True          # Grad clipping 사용 여부\n",
    "\n",
    "\n",
    "# Custom header\n",
    "class HeaderBlock(nn.Module):\n",
    "    def __init__(self, input_dim:int, output_dim:int, dropout_prob:float):\n",
    "        super(HeaderBlock, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(input_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.gelu(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def custom_header(x:int)->nn.Module:\n",
    "    \"\"\"\n",
    "    Pre-Activation Batch Normalization\n",
    "    - 깊은 Backbone 모델의 header이므로, Internal Covariate Shift 문제 해결을 위해 사용\n",
    "\n",
    "    BottleNeck\n",
    "    - 정보를 확장하여 중요한 정보만 남겨, 계산 효율성을 유지하면서 높은 표현력 제공\n",
    "    \"\"\"\n",
    "    header = nn.Sequential(\n",
    "        HeaderBlock(input_dim=x, output_dim=1024, dropout_prob=0.1),\n",
    "        HeaderBlock(input_dim=1024, output_dim=512, dropout_prob=0.3),\n",
    "        HeaderBlock(input_dim=512, output_dim=128, dropout_prob=0.5),\n",
    "        nn.Linear(128, 1)\n",
    "    )\n",
    "    return header\n",
    "\n",
    "CUSTOM_HEAD_FN = custom_header\n",
    "EXTRA_ACTIVATION_FN = \"sigmoid\"     # 모델의 출력 결과가 통과하는 추가 활성화 함수 존재\n",
    "\n",
    "# 모델 추론을 위한 metrics\n",
    "METRICS = MetricsClassification(is_binary=True, threshold=0.5)\n",
    "# 모델의 종류\n",
    "MODEL_TYPE = \"classification\"\n",
    "# 모델 log 설정\n",
    "LOG_INS = Log(log_dir=LOG, process_id=PROCESS_ID, model_type=MODEL_TYPE, save_log=SAVE_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. pipe line setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n",
      "GPU size: 2\n",
      "------------------------------------------------------------\n",
      "GPU number: 0\n",
      "Name: NVIDIA GeForce RTX 3080 Ti\n",
      "Computer capability: 8.6\n",
      "VRAM: 12GB\n",
      "------------------------------------------------------------\n",
      "GPU number: 1\n",
      "Name: NVIDIA GeForce GTX 750\n",
      "Computer capability: 5.0\n",
      "VRAM: 1GB\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# gpu 상태 확인\n",
    "GET_DEVICE = GetDevice()\n",
    "GET_DEVICE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device 설정\n",
    "GPU = 0\n",
    "DEVICE = GET_DEVICE(GPU)\n",
    "torch.cuda.set_device(DEVICE)\n",
    "\n",
    "# data loader 관련 설정\n",
    "DATASET_CLASS = ImgDataset           # dataset의 class\n",
    "TRAIN_DATASET = {\n",
    "    \"augments\":sample_augment, \n",
    "    \"resize\":224,\n",
    "    \"resize_how\":0,                  # resize 방법\n",
    "    \"resize_how_list\":[2, 3, 4],     # 무작위 resize 시, 방법의 list\n",
    "    \"resize_padding_color\":\"random\"  # # resize padding 시, pixel의 색\n",
    "}\n",
    "VALID_DATASET = {\n",
    "    \"resize\":224,\n",
    "    \"resize_how\":2,                  # resize 방법\n",
    "    \"resize_padding_color\":\"black\"  # # resize padding 시, pixel의 색\n",
    "}\n",
    "BATCH_SIZE = 16\n",
    "WORKER = 0                          # DataLoader의 num_worker\n",
    "\n",
    "# early stopping 시 경로\n",
    "ESTOP_PATH = f\"{SOURCE}/{ESPOINT_DIR}/process_{PROCESS_ID}\"\n",
    "\n",
    "# process 진행 중 생성되는 파일들이 저장되는 초기 디렉터리 초기화 여부\n",
    "MAKE_NEW_DEFAULT_DIR = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3. Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_DICT = {\n",
    "    'epochs':5,\n",
    "    'lr':0.0001,\n",
    "    'betas':(0.9, 0.999),\n",
    "    'eps':1e-08,\n",
    "    'clipping_max_norm':5\n",
    "}\n",
    "OPTIM_INS = Optim(name='Adam', hp_dict=HP_DICT)\n",
    "LOSS_FN = nn.BCEWithLogitsLoss()\n",
    "# label의 dtype을 loss function에 맞게 설정\n",
    "LABEL_DTYPE_INS = LabelDtype(loss_fn=LOSS_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "### Process 1. make key_df\n",
    "* key_df는 img의 절대 경로(\"path\")와 label(\"label\") 두 개의 컬럼으로 구성된 DataFrame 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 정보\n",
    "TRAIN_SET = \"/mnt/d/rawdata/dogs-vs-cats/train/\"        # train set의 경로\n",
    "TEST_SET = \"/mnt/d/rawdata/dogs-vs-cats/test1/\"         # test set의 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/d/rawdata/dogs-vs-cats/train/cat.0.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/d/rawdata/dogs-vs-cats/train/cat.1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/d/rawdata/dogs-vs-cats/train/cat.10.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/d/rawdata/dogs-vs-cats/train/cat.100.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/d/rawdata/dogs-vs-cats/train/cat.1000.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>/mnt/d/rawdata/dogs-vs-cats/train/dog.9995.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>/mnt/d/rawdata/dogs-vs-cats/train/dog.9996.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>/mnt/d/rawdata/dogs-vs-cats/train/dog.9997.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>/mnt/d/rawdata/dogs-vs-cats/train/dog.9998.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>/mnt/d/rawdata/dogs-vs-cats/train/dog.9999.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path  label\n",
       "0         /mnt/d/rawdata/dogs-vs-cats/train/cat.0.jpg      0\n",
       "1         /mnt/d/rawdata/dogs-vs-cats/train/cat.1.jpg      0\n",
       "2        /mnt/d/rawdata/dogs-vs-cats/train/cat.10.jpg      0\n",
       "3       /mnt/d/rawdata/dogs-vs-cats/train/cat.100.jpg      0\n",
       "4      /mnt/d/rawdata/dogs-vs-cats/train/cat.1000.jpg      0\n",
       "...                                               ...    ...\n",
       "24995  /mnt/d/rawdata/dogs-vs-cats/train/dog.9995.jpg      1\n",
       "24996  /mnt/d/rawdata/dogs-vs-cats/train/dog.9996.jpg      1\n",
       "24997  /mnt/d/rawdata/dogs-vs-cats/train/dog.9997.jpg      1\n",
       "24998  /mnt/d/rawdata/dogs-vs-cats/train/dog.9998.jpg      1\n",
       "24999  /mnt/d/rawdata/dogs-vs-cats/train/dog.9999.jpg      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key_df 생성\n",
    "path_list = GetAbsolutePath(None).get_all_path(parents_path=TRAIN_SET)\n",
    "key_df = make_basic_key_df(\n",
    "    paths=path_list,\n",
    "    labels=[re.split(r\".+/\", i, maxsplit=1)[1].split('.')[0] for i in path_list]\n",
    ")\n",
    "# label을 이진 분류로 변환\n",
    "key_df['label'] = binary_label_convertor(array=key_df['label'], positive_class='dog')\n",
    "\n",
    "# 이해를 돕기 위한 key_df 출력\n",
    "key_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 2. make idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAKE_NEW_IDX_DICT = True                            # idx_dict을 새로 생성할지 여부\n",
    "IDX_DICT_PATH = f\"{SOURCE}/idx_dict.pickle\"         # idx_dict이 저장될 경로\n",
    "\n",
    "# idx_dict 생성 방식\n",
    "K_SIZE = 5                  # k-fold의 크기 (Stratified sampling)\n",
    "TEST_RATIO = 0.2            # test dataset ratio\n",
    "VALID_RATIO = 0.1           # validation dataset ratio, None인 경우 생성하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기초 디렉터리 생성\n",
    "make_basic_directory(source=SOURCE, estop_dir=ESPOINT_DIR, log=LOG, result=RESULT, make_new=MAKE_NEW_DEFAULT_DIR)\n",
    "\n",
    "# idx_dict 생성\n",
    "IDX_DICT = do_or_load(\n",
    "    savepath=IDX_DICT_PATH, makes_new=MAKE_NEW_IDX_DICT, \n",
    "    fn=make_stratified_idx_dict,\n",
    "    key_df=key_df, stratified_columns=['label'], is_binary=True,\n",
    "    path_col='path', label_col='label', \n",
    "    k_size=K_SIZE, test_ratio=TEST_RATIO, valid_ratio=VALID_RATIO, \n",
    "    code_test=CODE_TEST\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 3. make option instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = Option(\n",
    "    process_id=PROCESS_ID, verbose=VERBOSE, log_ins=LOG_INS,\n",
    "\n",
    "    model_name=MODEL_NAME, pretrained=PRETRAINED, device=DEVICE, use_amp=USE_AMP, use_clipping=USE_CLIPPING,\n",
    "    img_channel=IMG_CHANNEL, class_size=CLASS_SIZE, custom_header=CUSTOM_HEAD_FN, extra_activation_fn=EXTRA_ACTIVATION_FN,\n",
    "    metrics=METRICS,\n",
    "\n",
    "    optimizer=OPTIM_INS, loss_fn=LOSS_FN, label_dtype_fn=LABEL_DTYPE_INS, \n",
    "    tuner_how=2, hp_dict=HP_DICT, \n",
    "\n",
    "    dataset_class=DATASET_CLASS, trainset_kwargs=TRAIN_DATASET, validset_kwargs=VALID_DATASET, batch_size=BATCH_SIZE, worker=WORKER,\n",
    "\n",
    "    idx_dict=IDX_DICT, results_parents=RESULT, espoint_parents=f\"{SOURCE}/{ESPOINT_DIR}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 4. model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 전 모든 seed 고정\n",
    "set_seed_everything(seed=SEED)\n",
    "# Log directory와 file 생성\n",
    "option.log_ins.make()\n",
    "\n",
    "# k-fold cross validation\n",
    "for k in option.idx_dict.keys():\n",
    "    k_idx_dict = option.idx_dict[k]     # k-fold에 대한 idx_dict\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "option.log_ins.k = k        # log의 k값 설정\n",
    "\n",
    "# Data Loader 정의\n",
    "loader = GetLoader(\n",
    "    idx_dict=k_idx_dict, dataset_class=option.dataset_class, \n",
    "    batch_size=option.batch_size, workers=option.worker\n",
    ")\n",
    "loader(key=\"train\", **option.trainset_kwargs)\n",
    "loader(key=\"test\", **option.validset_kwargs)\n",
    "if \"valid\" in k_idx_dict:\n",
    "    loader(key=\"valid\", **option.validset_kwargs)\n",
    "\n",
    "# model 정의\n",
    "model = Classification(\n",
    "    model_name=option.model_name, pretrained=option.pretrained, \n",
    "    channel=option.img_channel, class_size=option.class_size,\n",
    "    custom_head_fn=option.custom_header\n",
    ").to(option.device)\n",
    "\n",
    "# Optimizer 설정\n",
    "optimizer = option.optimizer(param=model.parameters())\n",
    "grad_scaler = torch.GradScaler(device=option.device) if option.use_amp else None\n",
    "back_propagation = BackPropagation(\n",
    "    optimizer=optimizer, use_amp=option.use_amp, grad_scaler=grad_scaler,\n",
    "    use_clipping=option.use_clipping, \n",
    "    max_norm=option.hp_dict['clipping_max_norm'] if 'clipping_max_norm' in option.hp_dict else None\n",
    ")\n",
    "\n",
    "# Fine tuning 방법 정의\n",
    "tuner = Tuner(model, how=2, freezing_ratio=0.9)\n",
    "tuner(epoch=0)      # model parameter 초기 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification:\n",
    "    def __init__(self, model, loader, optimizer, back_propagation, option):\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.optimizer = optimizer\n",
    "        self.back_propagation = back_propagation\n",
    "        self.option = option\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _fit_iterator(self, epoch_log_txt:str)->Tuple[float, float]:\n",
    "        pbar_ins = ProgressBar(header=\"Iterator \", time_log=True, memory_log=False, step=5, verbose=self.option.verbose)\n",
    "        stack_output = np.array([], dtype=np.float32)\n",
    "        stack_label = np.array([], dtype=np.float32)\n",
    "        loss_list = []\n",
    "\n",
    "        # 학습 모드로 설정\n",
    "        self.model.train()\n",
    "        for i, (pbar_txt, (imgs, labels)) in enumerate(pbar_ins(self.loader.train)):\n",
    "\n",
    "            # load to device\n",
    "            imgs = imgs.to(self.option.device)\n",
    "            # label dtype을 loss_fn에 맞게 수정 및 shape 등 변형\n",
    "            labels = self.option.label_dtype_fn(labels).reshape(-1, 1).to(self.option.device)\n",
    "            # 모델 학습 및 평활된 numpy 배열 출력\n",
    "            loss, output_array, label_array = self._fit_and_flatten_in_one_iter(imgs, labels)\n",
    "            # 결과 정리\n",
    "            loss_list.append(loss.item())\n",
    "            stack_output = np.concatenate((stack_output, output_array), axis=0)\n",
    "            stack_label = np.concatenate((stack_label, label_array), axis=0)\n",
    "            # Iterator 종료 시점을 기준으로 log 출력\n",
    "            _, _ = pbar_ins.end_of_iterator(progress_txt=f\"{epoch_log_txt}{pbar_txt}\", i=i)\n",
    "        \n",
    "        # iterator의 loss 평균, Accuracy출력\n",
    "        train_loss = np.mean(loss_list)\n",
    "        train_accuracy = self.option.metrics(predict=stack_output, label=stack_label, accuracy_only=True)\n",
    "        return train_loss, train_accuracy\n",
    "    \n",
    "\n",
    "    def _fit_and_flatten_in_one_iter(\n",
    "            self, imgs:torch.Tensor, labels:torch.Tensor\n",
    "        )->Tuple[torch.Tensor, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        모델을 학습하고, 추론된 결과와 label을 1차원 numpy 배열로 출력한다.\n",
    "\n",
    "        Args:\n",
    "            imgs (torch.Tensor): Tensor 이미지\n",
    "            labels (torch.Tensor): Tensor label\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, np.ndarray, np.ndarray]: loss, img 배열, label 배열\n",
    "        \"\"\"\n",
    "        # AMP\n",
    "        with torch.autocast(device_type=self.option.device, enabled=self.option.use_amp):\n",
    "            output = self.model(imgs)\n",
    "            loss = self.option.loss_fn(output, labels)\n",
    "            \n",
    "        # back propagation\n",
    "        self.back_propagation(loss)\n",
    "\n",
    "        # output과 label을 평활하고 numpy 배열로 변환\n",
    "        output = self.output_handler(tensor=output, extra_activation_fn=self.option.extra_activation_fn)\n",
    "        labels = self.output_handler(tensor=labels)\n",
    "\n",
    "        return loss, output, labels\n",
    "\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def inference(self, loader:data.DataLoader):\n",
    "        stack_output = np.array([], dtype=np.float32)\n",
    "        stack_label = np.array([], dtype=np.float32)\n",
    "        loss_list = []\n",
    "\n",
    "        # 추론 모드로 설정\n",
    "        self.model.eval()\n",
    "        for imgs, labels in loader:\n",
    "\n",
    "            imgs = imgs.to(self.option.device)\n",
    "            labels = self.option.label_dtype_fn(labels).reshape(-1, 1).to(self.option.device)\n",
    "            # 모델 추론 및 평활된 numpy 배열 출력\n",
    "            loss, output_array, label_array = self._inference_and_flatten(imgs, labels)\n",
    "            # 결과 정리\n",
    "            loss_list.append(loss.item()) \n",
    "            stack_output = np.concatenate((stack_output, output_array), axis=0)\n",
    "            stack_label = np.concatenate((stack_label, label_array), axis=0)\n",
    "            \n",
    "        # inference의 loss 평균, Accuracy출력\n",
    "        eval_loss = np.mean(loss_list)\n",
    "        eval_accuracy = self.option.metrics(predict=stack_output, label=stack_label, accuracy_only=True)\n",
    "        return eval_loss, eval_accuracy\n",
    "    \n",
    "\n",
    "    def _inference_and_flatten(\n",
    "            self, imgs:torch.Tensor, labels:torch.Tensor\n",
    "        )->Tuple[torch.Tensor, np.ndarray, np.ndarray]:\n",
    "        # GPU를 사용하는 동안 추론\n",
    "        with torch.no_grad():\n",
    "            output = self.model(imgs)\n",
    "            loss = self.option.loss_fn(output, labels)\n",
    "\n",
    "        # 각 배치마다 동기화 GPU 사용이 종료된 후 CPU 사용\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # output과 label을 평활하고 numpy 배열로 변환\n",
    "        output = self.output_handler(tensor=output, extra_activation_fn=self.option.extra_activation_fn)\n",
    "        labels = self.output_handler(tensor=labels)\n",
    "\n",
    "        return loss, output, labels\n",
    "\n",
    "\n",
    "    def output_handler(self, tensor:torch.Tensor, extra_activation_fn:Optional[str]=None)->np.ndarray:\n",
    "        \"\"\"\n",
    "        모델 추론 결과를 1차원 numpy 배열로 변환\n",
    "        >>> 모델의 추론 결과를 추가 활성화 함수(sigmoid, softmax)에 통과시킬 필요가 있다면, 그에 해당하는 활성화 함수에 통과시킴\n",
    "\n",
    "        Args:\n",
    "            output (torch.Tensor): 모델의 추론 결과\n",
    "            extra_activation_fn (Optional[str], optional): 추가 활성화 함수. Defaults to None.\n",
    "                - 'sigmoid', 'softmax' 존재\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 1 차원 배열로 변환된 모델의 추론 결과\n",
    "        \"\"\"\n",
    "        # extra_activateion_fn이 존재하는 경우, 그에 해당하는 활성화 함수를 통과 시킴.\n",
    "        act_fns = {'sigmoid':torch.sigmoid, 'softmax': lambda x: torch.softmax(x, dim=1)}\n",
    "        act_fn= act_fns.get(extra_activation_fn, None)\n",
    "        if act_fn is not None:\n",
    "            tensor = act_fn(tensor)\n",
    "        \n",
    "        # numpy로 전환\n",
    "        np_array = tensor.to('cpu').detach().numpy()\n",
    "        # 1 차원으로 평활\n",
    "        return np_array.flatten()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INS = Classification(\n",
    "    model=model, loader=loader, optimizer=optimizer, \n",
    "    back_propagation=back_propagation, option=option\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(\n",
    "            self, \n",
    "            patience:int=5, delta:float=0.0, mode='min', \n",
    "            verbose:bool=False, path:str='checkpoint.pt',\n",
    "            auto_remove:bool=True\n",
    "        ):\n",
    "        # 고정된 instance variable\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.path = path\n",
    "        self.auto_remove = auto_remove\n",
    "\n",
    "        # 변하는 instance variable\n",
    "        self.best_score = None          # 최고 점수\n",
    "        self.stop = False               # Early stopping으로 학습을 종료할지 여부\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, val_loss:float, model:torch.nn.Module):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    [#########################################]( 5/ 5)\n",
      "[Metrics] [Train]: acc: 1.0, loss: 0.0242 / [valid]: acc: 1.0, loss: 0.0156\n",
      "[time] spent: 0:02:08.74, eta: 0:00:25.74, 25.749s/it, current: 2024.11.19 17:43:34\n",
      "[memory] used:6.469 gb/62.707 gb(11.6 %), rest:55.430 gb, process:1.914 gb, cpu:50.7 %\n"
     ]
    }
   ],
   "source": [
    "pbar_ins = ProgressBar(header=\"Epoch    \", time_log=True, memory_log=True, step=1, verbose=0)\n",
    "\n",
    "acc_dict = {\"train\":[], \"valid\":[]}\n",
    "loss_dict = {\"train\":[], \"valid\":[]}\n",
    "\n",
    "epoch_log_txt = None\n",
    "for bar_txt, epoch in pbar_ins(range(option.hp_dict['epochs'])):\n",
    "\n",
    "    # Epochs 시작 시 log txt\n",
    "    epoch_log_txt = TEST_INS.option.log_ins.epoch_log_txt(epoch_log_txt=epoch_log_txt, bar_txt=bar_txt)\n",
    "\n",
    "    # model training\n",
    "    train_loss, train_acc = TEST_INS._fit_iterator(epoch_log_txt)\n",
    "    loss_dict['train'].append(train_loss)\n",
    "    acc_dict['train'].append(train_acc)\n",
    "\n",
    "    # validation\n",
    "    if TEST_INS.loader.valid is not None:\n",
    "        valid_loss, valid_acc = TEST_INS.inference(loader=TEST_INS.loader.valid)\n",
    "        loss_dict['valid'].append(train_loss)\n",
    "        acc_dict['valid'].append(train_acc)\n",
    "    else:\n",
    "        valid_loss, valid_acc = (None, None)\n",
    "\n",
    "    # 모델 성능에 대한 log txt\n",
    "    performance_txt = TEST_INS.option.log_ins.log_text(\n",
    "        train_acc=train_acc, train_loss=train_loss, valid_acc=valid_acc, valid_loss=valid_loss)\n",
    "\n",
    "    # epoch iteration 종료 시, 로그\n",
    "    epoch_log_txt, epoch_log_dict = pbar_ins.end_of_iterator(\n",
    "        progress_txt=bar_txt + \"\\n\" + performance_txt, i=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "# class Scheduler:\n",
    "#     def __init__(self, name:str, hp_dict:Dict[str, Any]):\n",
    "#         self.methods = {\n",
    "#             'LambdaLR': lr_scheduler.LambdaLR,\n",
    "#             'MultiplicativeLR': lr_scheduler.MultiplicativeLR,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             'StepLR': lr_scheduler.StepLR,\n",
    "#             'MultiStepLR': lr_scheduler.MultiStepLR,\n",
    "#             'ExponentialLR': lr_scheduler.ExponentialLR,\n",
    "#             'CosineAnnealingLR': lr_scheduler.CosineAnnealingLR,\n",
    "#             'CyclicLR': lr_scheduler.CyclicLR,\n",
    "#             'CosineAnnealingWarmRestarts': lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "#             'ReduceLROnPlateau': lr_scheduler.ReduceLROnPlateau,\n",
    "#             'OneCycleLR': lr_scheduler.OneCycleLR,\n",
    "#             'PolynomialLR': lr_scheduler.PolynomialLR,\n",
    "#             'LinearLR': lr_scheduler.LinearLR,\n",
    "#             'ConstantLR': lr_scheduler.ConstantLR,\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             'ChainedScheduler': lr_scheduler.ChainedScheduler,\n",
    "#             'SequentialLR': lr_scheduler.SequentialLR,\n",
    "            \n",
    "#         }\n",
    "\n",
    "\n",
    "#     def lambdalr(self):\n",
    "#         pass\n",
    "\n",
    "\n",
    "# class Schedulers:\n",
    "#     def __init__(self):\n",
    "#         pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_e4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
